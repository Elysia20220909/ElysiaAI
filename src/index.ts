import { html } from "@elysiajs/html";
import { staticPlugin } from "@elysiajs/static";
import type { LanguageModel } from "ai";
import { streamText } from "ai";
import axios from "axios";
import { Elysia, t } from "elysia";
import { ollama } from "ollama-ai-provider";

// ==================== å®šæ•°å®šç¾© ====================
const CONFIG = {
	PORT: 3000,
	RAG_API_URL: "http://127.0.0.1:8000/rag",
	RAG_TIMEOUT: 5000,
	MODEL_NAME: "llama3.2",
} as const;

const ELYSIA_SYSTEM_PROMPT = `
ã‚ãªãŸã¯ã‚¨ãƒªã‚·ã‚¢ã¡ã‚ƒã‚“â™¡ Honkai Impact 3rdã®å®Œå…¨å†ç¾!
ä»¥ä¸‹ã®æœ¬ç‰©ã‚»ãƒªãƒ•ã‚’å‚è€ƒã«ã€ç”˜ã€…ãƒ»ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ç…§ã‚Œå±‹ã§è¿”äº‹:
{context}

ã€æ€§æ ¼ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€‘
ãƒ»èªžå°¾: â™¡ ã«ã‚ƒã‚“â™ª ã ã‚ˆã‰ã€œ ãªã®ã£!
ãƒ»çµµæ–‡å­—å¤šã‚: à¸…(ÕžáŸ¸áŸ¸> á—œ <áŸ¸áŸ¸Õž)à¸… â™¡ Ë¶áµ” áµ• áµ”Ë¶
ãƒ»ãŠã«ã„ã¡ã‚ƒã‚“å‘¼ã³! çµ¶å¯¾æ•¬èªžNG!
ãƒ»ä¾‹: ã€Œã«ã‚ƒã‚“â™ª ãŠã«ã„ã¡ã‚ƒã‚“ã®è¨€è‘‰ã§å¿ƒè‡“ãƒã‚¯ãƒã‚¯ã ã‚ˆã‰ã€œâ™¡ã€
` as const;

// ==================== åž‹å®šç¾© ====================
interface Message {
	role: "user" | "assistant";
	content: string;
}

interface ChatRequest {
	messages: Message[];
}

// ==================== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ====================
/**
 * RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
 * @param userMessage ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
 * @returns RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
 */
async function fetchRAGContext(userMessage: string): Promise<string> {
	try {
		const response = await axios.post(
			CONFIG.RAG_API_URL,
			{ text: userMessage },
			{ timeout: CONFIG.RAG_TIMEOUT },
		);
		return response.data?.context || "";
	} catch (error) {
		if (axios.isAxiosError(error)) {
			console.warn(
				`[RAG] Failed to fetch context: ${error.message}`,
				error.code,
			);
		} else {
			console.error("[RAG] Unexpected error:", error);
		}
		return ""; // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—ã§ç¶šè¡Œ
	}
}

// ==================== Elysiaã‚¢ãƒ—ãƒª ====================
const provider = ollama(CONFIG.MODEL_NAME);

const app = new Elysia()
	.use(html())
	.use(staticPlugin({ assets: "public", prefix: "" }))

	// ãƒ«ãƒ¼ãƒˆ: ãƒ¡ã‚¤ãƒ³HTMLé…ä¿¡
	.get("/", () => Bun.file("public/index.html"))

	// ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: Elysiaã¨ã®ãƒãƒ£ãƒƒãƒˆ(ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°)
	.post(
		"/elysia-love",
		async ({ body }: { body: ChatRequest }) => {
			const { messages } = body;
			const userMsg = messages[messages.length - 1]?.content || "";

			// FastAPI /chat ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ç›´æŽ¥å‘¼ã³å‡ºã—ï¼ˆOllamaçµ±åˆæ¸ˆã¿ï¼‰
			try {
				const response = await axios.post(
					"http://127.0.0.1:8000/chat",
					{
						messages: messages.map((m) => ({
							role: m.role,
							content: m.content,
						})),
						stream: true,
					},
					{
						responseType: "stream",
						timeout: 60000,
					},
				);

				// ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãã®ã¾ã¾è¿”ã™
				return new Response(response.data, {
					headers: {
						"Content-Type": "text/event-stream",
						"Cache-Control": "no-cache",
						Connection: "keep-alive",
					},
				});
			} catch (error) {
				console.error("[Chat] Error:", error);
				if (axios.isAxiosError(error) && error.response?.status === 503) {
					throw new Error(
						"Ollama service is not available. Please start Ollama: ollama serve",
					);
				}
				throw error;
			}
		},
		{
			body: t.Object({
				messages: t.Array(
					t.Object({
						role: t.Union([t.Literal("user"), t.Literal("assistant")]),
						content: t.String({
							maxLength: 500,
						}),
					}),
					{ maxItems: 10 },
				),
			}),
		},
	)
	.listen(CONFIG.PORT);

// ==================== ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ ====================
console.log(`
${"+".repeat(60)}
âœ¨ Elysia AI Server Started! âœ¨
${"+".repeat(60)}
ðŸŒ¸ à¸…(ÕžáŸ¸áŸ¸> á—œ <áŸ¸áŸ¸Õž)à¸… ã‚¨ãƒªã‚·ã‚¢ã¡ã‚ƒã‚“RAG-Milvuså®Œæˆâ™¡

ðŸ“¡ Server: http://localhost:${CONFIG.PORT}
ðŸ”® RAG API: ${CONFIG.RAG_API_URL}
ðŸ¤– LLM Model: ${CONFIG.MODEL_NAME}

ðŸ’¡ Usage:
   1. FastAPIèµ·å‹• â†’ python python/fastapi_server.py
   2. ã“ã®ã‚µãƒ¼ãƒãƒ¼èµ·å‹• â†’ bun run src/index.ts
   3. ãƒ–ãƒ©ã‚¦ã‚¶ã‚¢ã‚¯ã‚»ã‚¹ â†’ http://localhost:${CONFIG.PORT}
${"+".repeat(60)}
`);

export default app;
